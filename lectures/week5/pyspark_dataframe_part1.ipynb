{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tutorial of json operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you want to import json\n",
    "import json\n",
    "\n",
    "# this is a dictionary, python dictionary\n",
    "demo_data = {\"name\": \"Bob\",\"languages\": [\"English\", \"Fench\"],\"married\": True,\"age\": 32}\n",
    "\n",
    "demo_json_path = './data/json_demo.json'\n",
    "\n",
    "# this shows how to save python object to json file\n",
    "with open(demo_json_path, 'w') as f:\n",
    "    json.dump(demo_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Bob\", \"languages\": [\"English\", \"Fench\"], \"married\": true, \"age\": 32}"
     ]
    }
   ],
   "source": [
    "!cat ./data/json_demo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Bob', 'languages': ['English', 'Fench'], 'married': True, 'age': 32}\n"
     ]
    }
   ],
   "source": [
    "# here shows how to read json file and create python object\n",
    "with open(demo_json_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}"
     ]
    }
   ],
   "source": [
    "!cat ./data/people.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to Spark DataFrame         \n",
    "## Semi-structured Data\n",
    "## Create SqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file into a DataFrame\n",
    "from pyspark import SQLContext\n",
    "\n",
    "#Start an sqlContext\n",
    "sqlContext = SQLContext.getOrCreate(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Json\n",
    "people_df = sqlContext.read.json(\"./data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is python object [{'age': None, 'name': 'Michael'}, {'age': 30, 'name': 'Andy'}, {'age': 19, 'name': 'Justin'}]\n",
      "this is a rdd in slave node ParallelCollectionRDD[38] at parallelize at PythonRDD.scala:195\n",
      "this is all the rdd data [{'age': None, 'name': 'Michael'}, {'age': 30, 'name': 'Andy'}, {'age': 19, 'name': 'Justin'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# this is python object\n",
    "people_python_object = [{'age':None, 'name':'Michael'},\n",
    " {'age':30, 'name':'Andy'},\n",
    " {'age':19, 'name':'Justin'}]\n",
    "print('this is python object', people_python_object)\n",
    "\n",
    "# this Spark RDD, create rdd from python object\n",
    "people_rdd = sc.parallelize(people_python_object)\n",
    "print('this is a rdd in slave node', people_rdd)\n",
    "print('this is all the rdd data', people_rdd.collect())\n",
    "\n",
    "# create dataframe from RDD\n",
    "people_df = people_rdd.toDF(['age','name'])\n",
    "people_df.show()\n",
    "\n",
    "# get rdd from dataframe\n",
    "people_df_rdd = people_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before you do sql query, create TempView and give it a name\n",
    "people_df.createOrReplaceTempView(\"people_table\")\n",
    "\n",
    "#Perform SQL Query, this is the simplest SQL query\n",
    "all_people_df = spark.sql(\"SELECT * FROM people_table WHERE age > 20\")\n",
    "all_people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|word           |count|\n",
      "+---------------+-----+\n",
      "|is             |59   |\n",
      "|term           |7    |\n",
      "|non-traditional|1    |\n",
      "|needed         |2    |\n",
      "|gather         |2    |\n",
      "|organize       |2    |\n",
      "|process        |15   |\n",
      "|large          |18   |\n",
      "|datasets       |11   |\n",
      "|of             |111  |\n",
      "|working        |4    |\n",
      "|power          |1    |\n",
      "|storage        |9    |\n",
      "|single         |7    |\n",
      "|new            |4    |\n",
      "|value          |5    |\n",
      "|this           |13   |\n",
      "|type           |5    |\n",
      "|in             |41   |\n",
      "|years          |1    |\n",
      "|               |38   |\n",
      "|we             |7    |\n",
      "|fundamental    |1    |\n",
      "|common         |4    |\n",
      "|concepts       |3    |\n",
      "|researching    |1    |\n",
      "|subject        |1    |\n",
      "|take           |3    |\n",
      "|high-level     |1    |\n",
      "|look           |2    |\n",
      "|at             |8    |\n",
      "|processes      |4    |\n",
      "|used           |13   |\n",
      "|space          |2    |\n",
      "|What           |2    |\n",
      "|Data?          |1    |\n",
      "|down           |2    |\n",
      "|projects       |6    |\n",
      "|vendors        |1    |\n",
      "|business       |2    |\n",
      "|professionals  |1    |\n",
      "|use            |2    |\n",
      "|quite          |2    |\n",
      "|mind           |2    |\n",
      "|category       |1    |\n",
      "|are            |35   |\n",
      "|handle         |4    |\n",
      "|means          |5    |\n",
      "|store          |2    |\n",
      "|traditional    |5    |\n",
      "|tooling        |1    |\n",
      "|constantly     |2    |\n",
      "|shifting       |1    |\n",
      "|may            |2    |\n",
      "|significantly  |4    |\n",
      "|The            |18   |\n",
      "|basic          |1    |\n",
      "|requirements   |7    |\n",
      "|as             |17   |\n",
      "|size           |1    |\n",
      "|However        |2    |\n",
      "|massive        |1    |\n",
      "|speed          |3    |\n",
      "|characteristics|3    |\n",
      "|must           |1    |\n",
      "|stage          |2    |\n",
      "|significant    |2    |\n",
      "|challenges     |3    |\n",
      "|when           |5    |\n",
      "|designing      |2    |\n",
      "|goal           |1    |\n",
      "|systems        |24   |\n",
      "|connections    |1    |\n",
      "|volumes        |1    |\n",
      "|would          |2    |\n",
      "|possible       |2    |\n",
      "|using          |6    |\n",
      "|conventional   |3    |\n",
      "|Laney          |1    |\n",
      "|presented      |2    |\n",
      "|known          |2    |\n",
      "|three          |3    |\n",
      "|Vs             |2    |\n",
      "|describe       |2    |\n",
      "|make           |5    |\n",
      "|different      |6    |\n",
      "|other          |9    |\n",
      "|processed      |8    |\n",
      "|These          |5    |\n",
      "|orders         |1    |\n",
      "|magnitude      |1    |\n",
      "|larger         |1    |\n",
      "|than           |3    |\n",
      "|demands        |2    |\n",
      "|more           |11   |\n",
      "|thought        |1    |\n",
      "|cycle          |2    |\n",
      "|work           |9    |\n",
      "|exceed         |1    |\n",
      "|challenge      |2    |\n",
      "+---------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "input_data_path = '../week1/big_data_intro.txt'\n",
    "text_file = sc.textFile(input_data_path)\n",
    "counts_rdd = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts_df = counts_rdd.toDF(['word','count'])\n",
    "counts_df.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|        is|   59|\n",
      "|        of|  111|\n",
      "|        in|   41|\n",
      "|          |   38|\n",
      "|       are|   35|\n",
      "|   systems|   24|\n",
      "|    Apache|   22|\n",
      "|      data|  126|\n",
      "|         a|   66|\n",
      "|       for|   42|\n",
      "|       the|  145|\n",
      "|       and|  103|\n",
      "|        to|   87|\n",
      "|      with|   26|\n",
      "|      that|   32|\n",
      "| computing|   21|\n",
      "|       big|   35|\n",
      "|processing|   26|\n",
      "|        be|   28|\n",
      "|       can|   33|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_df.createOrReplaceTempView(\"count_table\")\n",
    "filtered_count_df = spark.sql(\"SELECT * FROM count_table WHERE count > 20\")\n",
    "filtered_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
