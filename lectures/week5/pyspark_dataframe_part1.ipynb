{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tutorial of json operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "demo_data = {\"name\": \"Bob\",\"languages\": [\"English\", \"Fench\"],\"married\": True,\"age\": 32}\n",
    "demo_json_path = './data/json_demo.json'\n",
    "with open(demo_json_path, 'w') as f:\n",
    "    json.dump(demo_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Bob\", \"languages\": [\"English\", \"Fench\"], \"married\": true, \"age\": 32}"
     ]
    }
   ],
   "source": [
    "!cat ./data/json_demo.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Bob', 'languages': ['English', 'Fench'], 'married': True, 'age': 32}\n"
     ]
    }
   ],
   "source": [
    "with open(demo_json_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    print(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}"
     ]
    }
   ],
   "source": [
    "!cat ./data/people.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to Spark DataFrame         \n",
    "## Semi-structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file into a DataFrame\n",
    "from pyspark import SQLContext\n",
    "\n",
    "#Start an sqlContext\n",
    "sqlContext = SQLContext.getOrCreate(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Json\n",
    "people_df = sqlContext.read.json(\"./data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.createOrReplaceTempView(\"people_table\")\n",
    "\n",
    "#Perform SQL Query\n",
    "all_people_df = spark.sql(\"SELECT * FROM people_table\")\n",
    "all_people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|name           |age|\n",
      "+---------------+---+\n",
      "|is             |59 |\n",
      "|term           |7  |\n",
      "|non-traditional|1  |\n",
      "|needed         |2  |\n",
      "|gather         |2  |\n",
      "|organize       |2  |\n",
      "|process        |15 |\n",
      "|large          |18 |\n",
      "|datasets       |11 |\n",
      "|of             |111|\n",
      "+---------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "input_data_path = '../week1/big_data_intro.txt'\n",
    "text_file = sc.textFile(input_data_path)\n",
    "counts_rdd = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts_df = counts_rdd.toDF(['name','age'])\n",
    "counts_df.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
