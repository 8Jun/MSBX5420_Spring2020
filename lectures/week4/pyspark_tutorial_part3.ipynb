{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Introduction', 12), ('Big', 3), ('data', 4), ('is', 2), ('a', 1), ('blanket', 7), ('term', 4), ('for', 3), ('the', 3), ('non-traditional', 15)]\n"
     ]
    }
   ],
   "source": [
    "input_data_path = '../week1/big_data_intro.txt'\n",
    "text_file = sc.textFile(input_data_path)\n",
    "\n",
    "word_to_length_pair_rdd = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, len(word)))\n",
    "print(word_to_length_pair_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction', 'Big', 'data', 'is', 'a', 'blanket', 'term', 'for', 'the', 'non-traditional']\n"
     ]
    }
   ],
   "source": [
    "word_to_length_pair_keys_rdd = word_to_length_pair_rdd.keys()\n",
    "print(word_to_length_pair_keys_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 3, 4, 2, 1, 7, 4, 3, 3, 15]\n"
     ]
    }
   ],
   "source": [
    "word_to_length_pair_values_rdd = word_to_length_pair_rdd.values()\n",
    "print(word_to_length_pair_values_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', <pyspark.resultiterable.ResultIterable object at 0x7f2c242462b0>), ('term', <pyspark.resultiterable.ResultIterable object at 0x7f2c242467f0>), ('non-traditional', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246b70>), ('needed', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246908>), ('gather', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246ac8>), ('organize', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246c50>), ('process', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246da0>), ('large', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246978>), ('datasets', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246f98>), ('of', <pyspark.resultiterable.ResultIterable object at 0x7f2c24246c18>)]\n"
     ]
    }
   ],
   "source": [
    "word_to_length_pair_group_rdd = word_to_length_pair_rdd.groupByKey()\n",
    "print(word_to_length_pair_group_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 118), ('term', 28), ('non-traditional', 15), ('needed', 12), ('gather', 12), ('organize', 16), ('process', 105), ('large', 90), ('datasets', 88), ('of', 222)]\n"
     ]
    }
   ],
   "source": [
    "word_to_length_pair_reduce_rdd = word_to_length_pair_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(word_to_length_pair_reduce_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Introduction', 1), ('Big', 12), ('data', 126), ('a', 66), ('blanket', 1), ('for', 42), ('the', 145), ('strategies', 4), ('and', 103), ('technologies', 5)]\n",
      "[('', 186), ('The', 149), ('Tragedy', 1), ('of', 623), ('Prince', 1), ('Denmark', 9), ('Shakespeare', 1), ('|', 2), ('Entire', 1), ('ACT', 5)]\n"
     ]
    }
   ],
   "source": [
    "bigdata_word_to_count_pair_rdd = sc.textFile('../week1/big_data_intro.txt') \\\n",
    "    .flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "#bigdata_word_to_count_pair_rdd = bigdata_word_to_count_pair_rdd.repartition(len(bigdata_word_to_count_pair_rdd.collect()))\n",
    "\n",
    "print(bigdata_word_to_count_pair_rdd.take(10))\n",
    "\n",
    "hamlet_word_to_count_pair_rdd = sc.textFile('./hamlet.txt') \\\n",
    "    .flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "#hamlet_word_to_count_pair_rdd = hamlet_word_to_count_pair_rdd.repartition(len(hamlet_word_to_count_pair_rdd.collect()))\n",
    "\n",
    "print(hamlet_word_to_count_pair_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata_join_hamlet_rdd = bigdata_word_to_count_pair_rdd.join(hamlet_word_to_count_pair_rdd)\n",
    "print(bigdata_join_hamlet_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
