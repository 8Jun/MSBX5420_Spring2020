{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing StreamingContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.\n",
    "A StreamingContext object can be created from a SparkContext object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext(master, appName)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After a context is defined, you have to do the following.\n",
    "Define the input sources by creating input DStreams.\n",
    "Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "Start receiving data and processing it using streamingContext.start().\n",
    "Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "The processing can be manually stopped using streamingContext.stop().\n",
    "\n",
    "## Points to remember:\n",
    "Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "Once a context has been stopped, it cannot be restarted.\n",
    "Only one StreamingContext can be active in a JVM at the same time.\n",
    "stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.\n",
    "A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Input DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Directories are Monitored\n",
    "Spark Streaming will monitor the directory dataDirectory and process any files created in that directory.\n",
    "\n",
    "A simple directory can be monitored, such as \"hdfs://namenode:8040/logs/\". All files directly under such a path will be processed as they are discovered.\n",
    "A POSIX glob pattern can be supplied, such as \"hdfs://namenode:8040/logs/2017/*\". Here, the DStream will consist of all files in the directories matching the pattern. That is: it is a pattern of directories, not of files in directories.\n",
    "All files must be in the same data format.\n",
    "A file is considered part of a time period based on its modification time, not its creation time.\n",
    "Once processed, changes to a file within the current window will not cause the file to be reread. That is: updates are ignored.\n",
    "The more files under a directory, the longer it will take to scan for changes â€” even if no files have been modified.\n",
    "If a wildcard is used to identify directories, such as \"hdfs://namenode:8040/logs/2016-*\", renaming an entire directory to match the path will add the directory to the list of monitored directories. Only the files in the directory whose modification time is within the current window will be included in the stream.\n",
    "Calling FileSystem.setTimes() to fix the timestamp is a way to have the file picked up in a later window, even if its contents have not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dir = 'hdfs://msbx5420-m/user/peter/spark_streaming_demo/'\n",
    "lines = ssc.textFileStream(hdfs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DStream Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda x: (x, 1))\\\n",
    "              .reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Operations on DStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start receiving data and processing it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
